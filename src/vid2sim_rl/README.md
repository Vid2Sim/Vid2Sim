# ðŸŽ¬ Vid2Sim ðŸ¤–: Realistic and Interactive Simulation from Video for Urban Navigation
> [Ziyang Xie](https://ziyangxie.site/), [Zhizheng Liu](https://scholar.google.com/citations?user=Asc7j9oAAAAJ&hl=en), [Zhenghao Peng](https://pengzhenghao.github.io/), [Wayne Wu](https://wywu.github.io/), [Bolei Zhou](https://boleizhou.github.io/)
>
> [![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org/abs/2501.06693)
> [![Project Page](https://img.shields.io/badge/Project-Page-blue)](https://metadriverse.github.io/vid2sim/)

Vid2Sim is a novel framework that converts monocular videos into photorealistic and physically interactive simulation environments for training embodied agents with minimal sim-to-real gap.

> **Info**: This subfolder is used for agent RL training in the reconstructed simulation environment.  
Please follow the instructions below to build the environment and run the RL training.    
*For simulation environment reconstruction, please refer to the [Vid2Sim-Recon](https://github.com/Vid2Sim/Vid2Sim/src/vid2sim_recon) subfolder.*


<p align="center">
  <img src="../../assets/teaser.png" width="100%">
</p>